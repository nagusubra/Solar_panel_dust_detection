{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONnv2IlOkG/JoYWny0hTZZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagusubra/Solar_panel_dust_detection/blob/main/Solar_panel_dust_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install libraries and modules"
      ],
      "metadata": {
        "id": "SQAMdUOp9lRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PLxJiY6u9Zh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25356798-ffc4-4d3a-f4a5-f011d93db05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/238.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m235.5/238.9 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.9/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.9/dist-packages (from openpyxl) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.9-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-model-optimization\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "\n",
        "import xlsxwriter\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "\n",
        "import tempfile\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "qeqtoyTCppFF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google drive (if you are using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WVnbwh-9x2l",
        "outputId": "e5bdc72e-8395-4ef9-b25c-3d2345905c0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "\n",
        "# mypath = '/content/drive/MyDrive/Solar_panel_dust_detection/dataset_2/dirty'\n",
        "\n",
        "# onlyfiles = [ f  for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "# onlyfiles"
      ],
      "metadata": {
        "id": "z1LwhLkSBQ3u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "\n",
        "# mypath = '/content/drive/MyDrive/Solar_panel_dust_detection/dataset_2/dirty'\n",
        "\n",
        "# onlyfiles = [ os.rename(join(mypath, f), join(mypath, f.split(\".\")[0] + \"_dirty\" + \".jpg\"))  for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "# onlyfiles"
      ],
      "metadata": {
        "id": "bwArxFhKADwi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation function"
      ],
      "metadata": {
        "id": "YjpFrB77S6QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Size\n",
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "  return os.path.getsize(zipped_file)\n",
        "\n",
        "def evaluate_model(model_path, model_info, val_dataset):\n",
        "\n",
        "  # Evaluate test accuracy and test loss\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "  test_loss, test_acc = model.evaluate(val_dataset, verbose=0)\n",
        "\n",
        "  # Evaluate Model Size\n",
        "  model_size = get_gzipped_model_size(model_path)\n",
        "\n",
        "  # Evaluate Inference Time\n",
        "  startTime = time.time()\n",
        "  prediction = model.predict(val_dataset)\n",
        "  executionTime = (time.time() - startTime)/len(val_dataset)\n",
        "\n",
        "  # Print\n",
        "  print('\\nModel Accuracy:', test_acc*100, '%')\n",
        "  print(\"Model Size: %.2f bytes\" % (model_size))\n",
        "  print(\"Inference Time is: \", executionTime, \"s\")\n",
        "\n",
        "  # Build Evalution dataframe\n",
        "  evulation_dict = {\n",
        "                      \"Evaluation type\": \"Evualation\",\n",
        "                      \"Model Information\": model_info,\n",
        "                      \"Accuracy\": str(test_acc*100) + \" %\",\n",
        "                      \"Loss\": str(test_loss*100) + \" %\",\n",
        "                      \"Model Size\": str(model_size) + \" bytes\",\n",
        "                      \"Inference Time\": str(executionTime) + \" sec\"\n",
        "                    }\n",
        "  \n",
        "  evulation_df = pd.DataFrame.from_dict(evulation_dict, orient='index').reset_index()\n",
        "\n",
        "\n",
        "  return test_acc, model_size, executionTime, evulation_df"
      ],
      "metadata": {
        "id": "oB55zL05S_Oe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build base model"
      ],
      "metadata": {
        "id": "vfkTB4_DiYTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SolNet(in_size):\n",
        "  i = Input(in_size)\n",
        "  r = Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(i)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Flatten()(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  o = Dense(1, activation='sigmoid')(r)\n",
        "  SolNet = Model(i, o)\n",
        "  SolNet.save('models/solnet.hdf5')\n",
        "  SolNet.compile(optimizer=Adam(.0001, .8, .9), loss=binary_crossentropy, metrics=['acc'])\n",
        "  SolNet.summary()\n",
        "  return SolNet"
      ],
      "metadata": {
        "id": "4gVkkz__icfY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Base Model"
      ],
      "metadata": {
        "id": "4ZeT32zWUEnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SolNet(in_size):\n",
        "  i = Input(in_size)\n",
        "  r = Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(i)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Flatten()(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  o = Dense(1, activation='sigmoid')(r)\n",
        "  SolNet = Model(i, o)\n",
        "  SolNet.save('models/solnet.hdf5')\n",
        "  SolNet.save('/content/drive/MyDrive/Solar_panel_dust_detection/models/solnet'+str(datetime.datetime.today().date())+'.hdf5')\n",
        "  SolNet.save('/content/drive/MyDrive/Solar_panel_dust_detection/models/solnet'+str(datetime.datetime.today().date())+'.h5')\n",
        "  SolNet.compile(optimizer=Adam(.0001, .8, .9), loss=binary_crossentropy, metrics=['acc'])\n",
        "  SolNet.summary()\n",
        "  return SolNet"
      ],
      "metadata": {
        "id": "8nc6lZnKUixz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  path = \"/content/models/solnet.hdf5\"\n",
        "  solnet = load_model(path, compile=False)\n",
        "  # history = solnet.history()\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.title('acc loss vs epoch')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'acc'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dnGGfkkgUo63"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Size\n",
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "QA55ZOpsaMiR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # base model\n",
        "\n",
        "# batch_size = 32\n",
        "# #location = 'dataset/'\n",
        "# location = \"/content/drive/MyDrive/Solar_panel_dust_detection/dataset_1\"\n",
        "# label_mode = 'binary'\n",
        "# seed = 10 #changed for each fold made manually\n",
        "# epochs=30\n",
        "# #epochs=5\n",
        "\n",
        "\n",
        "# class_names = ['clean', 'dirty']\n",
        "# in_size = [227, 227, 3]\n",
        "\n",
        "# tr_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "#                                           seed=seed, labels='inferred', image_size=in_size[:-1], \n",
        "#                                           subset = 'training', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "# val_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "#                                           seed=seed, labels='inferred', image_size=in_size[:-1],\n",
        "#                                           subset = 'validation', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "# in_size = [227, 227, 3]\n",
        "# SolNet = SolNet(in_size)\n",
        "\n",
        "\n",
        "\n",
        "# startTime = time.time()\n",
        "# history = SolNet.fit(tr_dataset, validation_data=val_dataset, epochs=epochs, batch_size=batch_size)\n",
        "# executionTime = (time.time() - startTime)"
      ],
      "metadata": {
        "id": "fNHot2r1TJdo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(history.history)"
      ],
      "metadata": {
        "id": "d4C6_Bb-Yi7w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate()"
      ],
      "metadata": {
        "id": "IJOz2CyMbpef"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(history.history['acc'], label='accuracy')\n",
        "# plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
        "# plt.title('model accuracy plot')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend(loc='best')\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss plot')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'val'], loc='best')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ei6SYOpNYk0n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Inference Time is\", executionTime/60, \"mins\")"
      ],
      "metadata": {
        "id": "4lFSom8Hg-ut"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updated Solnet model"
      ],
      "metadata": {
        "id": "khzhvYrY2KLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SolNet_updated(in_size, learning_rate, model_path):\n",
        "  i = Input(in_size)\n",
        "  r = Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(i)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Flatten()(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  o = Dense(1, activation='sigmoid')(r)\n",
        "\n",
        "\n",
        "  SolNet = Model(inputs = i, outputs = o)\n",
        "\n",
        "\n",
        "\n",
        "  # SolNet.save('models/solnet.hdf5')\n",
        "  # SolNet.save('/content/drive/MyDrive/Solar_panel_dust_detection/models/solnet_updated_'+str(datetime.datetime.today().date())+'.hdf5')\n",
        "  SolNet.compile(optimizer=Adam(learning_rate), loss=binary_crossentropy, metrics=['acc'])\n",
        "  SolNet.save(model_path)\n",
        "  SolNet.summary()\n",
        "  return SolNet"
      ],
      "metadata": {
        "id": "ZeVlShut2Rsc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = '/content/drive/MyDrive/Solar_panel_dust_detection/models/solnet_updated_'+ str(datetime.datetime.today().date()) + '.h5'\n",
        "# model_plot_path = '/content/drive/MyDrive/Solar_panel_dust_detection/plots/'+ str(datetime.datetime.today().date())\n",
        "# model_excel_file_path = '/content/drive/MyDrive/Solar_panel_dust_detection/evaluation/'+ str(datetime.datetime.today().date())\n",
        "# location = \"/content/drive/MyDrive/Solar_panel_dust_detection/dataset_1\"\n",
        "\n",
        "# batch_size = 32\n",
        "# label_mode = 'binary'\n",
        "# seed = 10 #changed for each fold made manually\n",
        "# epochs= 2\n",
        "# in_size = [227, 227, 3]\n",
        "\n",
        "# class_names = ['clean', 'dirty']\n",
        "# test_labels = class_names\n",
        "\n",
        "# tr_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "#                                           seed=seed, labels='inferred', image_size=in_size[:-1], \n",
        "#                                           subset = 'training', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "# val_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "#                                           seed=seed, labels='inferred', image_size=in_size[:-1],\n",
        "#                                           subset = 'validation', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "# # in_size = [227, 227, 3]\n",
        "# SolNet_model = SolNet_updated(in_size, 0.00001, model_path)\n",
        "\n",
        "\n",
        "\n",
        "# startTime = time.time()\n",
        "# history = SolNet_model.fit(tr_dataset, validation_data=val_dataset, epochs=epochs, batch_size=batch_size)\n",
        "# executionTime = (time.time() - startTime)"
      ],
      "metadata": {
        "id": "WChxOJSP8OYD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_info = \"batch_size: \" + str(batch_size) + \" and epochs: \" + str(epochs) + \" and in_size: \" + str(in_size)\n",
        "# print(model_info)\n",
        "# print(\"Inference Time is\", executionTime/60, \"mins\")"
      ],
      "metadata": {
        "id": "AMm00dJYTyZb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(history.history['acc'], label='accuracy')\n",
        "# plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
        "# plt.title('model accuracy plot')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend(loc='best')\n",
        "# plt.savefig(model_plot_path + '_model_accuracy_plot.png')\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss plot')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'val'], loc='best')\n",
        "# plt.savefig(model_plot_path + '_model_loss_plot.png')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "kPHxMTjn8byV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate Inference Time\n",
        "# startTime = time.time()\n",
        "# model = tf.keras.models.load_model(model_path)\n",
        "# test_loss, test_acc = model.evaluate(val_dataset, verbose=0)\n",
        "# prediction = model.predict(val_dataset)\n",
        "# executionTime = (time.time() - startTime)/len(val_dataset)\n",
        "# print(executionTime)\n",
        "# print(test_acc)"
      ],
      "metadata": {
        "id": "-ImC5vO8jfyh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_acc, model_size, executionTime, evulation_df = evaluate_model(model_path, model_info, val_dataset)"
      ],
      "metadata": {
        "id": "Lc9thO3I1sQn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evulation_df"
      ],
      "metadata": {
        "id": "qmzX6SNIsDwy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = model_excel_file_path + '_model_evaluation.xlsx'\n",
        "\n",
        "# with pd.ExcelWriter(path) as writer:\n",
        "#   # writer.book = openpyxl.load_workbook(path)\n",
        "#   evulation_df.to_excel(writer, sheet_name='Model_evaluation')\n",
        "#   pd.DataFrame().to_excel(writer, sheet_name='Plots')\n",
        "  \n",
        "\n",
        "#   worksheet = writer.sheets['Plots']\n",
        "#   worksheet.insert_image('A1', model_plot_path + '_model_accuracy_plot.png')\n",
        "#   worksheet.insert_image('K1', model_plot_path + '_model_loss_plot.png')\n",
        "#   writer.save()"
      ],
      "metadata": {
        "id": "tQ0Lf3_2b7Fu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loop based hyperparameter optimization"
      ],
      "metadata": {
        "id": "6RZntO1mdEGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SolNet_updated_for_loop(in_size, learning_rate, model_path):\n",
        "  i = Input(in_size)\n",
        "  r = Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(i)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Flatten()(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  o = Dense(1, activation='sigmoid')(r)\n",
        "\n",
        "\n",
        "  SolNet = Model(inputs = i, outputs = o)\n",
        "\n",
        "\n",
        "\n",
        "  # SolNet.save('models/solnet.hdf5')\n",
        "  # SolNet.save('/content/drive/MyDrive/Solar_panel_dust_detection/models/solnet_updated_'+str(datetime.datetime.today().date())+'.hdf5')\n",
        "  SolNet.compile(optimizer=Adam(learning_rate), loss=binary_crossentropy, metrics=['acc'])\n",
        "  SolNet.save(model_path)\n",
        "  SolNet.summary()\n",
        "  return SolNet"
      ],
      "metadata": {
        "id": "5ILEZZhbDSCe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, model_plot_path, iteration_number):\n",
        "  \n",
        "  model_accuracy_plot_modified = model_plot_path + \"_model_iteration_\" + str(iteration_number) + \"_model_accuracy_plot.png\"\n",
        "  model_loss_plot_modified = model_plot_path + \"_model_iteration_\" + str(iteration_number) + \"_model_loss_plot.png\"\n",
        "  \n",
        "  plt.plot(history.history['acc'], label='accuracy')\n",
        "  plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
        "  plt.title('model accuracy plot')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(loc='best')\n",
        "  \n",
        "  plt.savefig(model_accuracy_plot_modified)\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss plot')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='best')\n",
        "  plt.savefig(model_loss_plot_modified)\n",
        "  plt.show()\n",
        "\n",
        "  return model_accuracy_plot_modified, model_loss_plot_modified"
      ],
      "metadata": {
        "id": "QX-m2bZe9HQX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_execution_trial(evluation_df, model_excel_file_path, model_accuracy_plot_modified, model_loss_plot_modified, iteration_number):\n",
        "  path = model_excel_file_path + \"_model_iteration_\" + str(iteration_number) + '_model_evaluation.xlsx'\n",
        "\n",
        "  with pd.ExcelWriter(path) as writer:\n",
        "    # writer.book = openpyxl.load_workbook(path)\n",
        "    evluation_df.to_excel(writer, sheet_name='Model_evaluation')\n",
        "    pd.DataFrame().to_excel(writer, sheet_name='Plots')\n",
        "    \n",
        "\n",
        "    worksheet = writer.sheets['Plots']\n",
        "    worksheet.insert_image('A1', model_accuracy_plot_modified)\n",
        "    worksheet.insert_image('K1', model_loss_plot_modified)\n",
        "    writer.save()"
      ],
      "metadata": {
        "id": "z-IHWYoWAOYF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/Solar_panel_dust_detection/models/solnet_updated_'+ str(datetime.datetime.today().date())\n",
        "model_plot_path = '/content/drive/MyDrive/Solar_panel_dust_detection/plots/'+ str(datetime.datetime.today().date())\n",
        "model_excel_file_path = '/content/drive/MyDrive/Solar_panel_dust_detection/evaluation/'+ str(datetime.datetime.today().date())\n",
        "location = \"/content/drive/MyDrive/Solar_panel_dust_detection/dataset_1\"\n",
        "\n",
        "\n",
        "\n",
        "loop_dict = {\n",
        "              # index: [ bs, epoch, lr]\n",
        "              1:       [ 32, 10,    0.0001  ],\n",
        "              2:       [ 32, 10,    0.00001 ],\n",
        "              3:       [ 32, 20,    0.0001  ],\n",
        "              4:       [ 32, 20,    0.00001 ],\n",
        "              5:       [ 64, 10,    0.0001  ],\n",
        "              6:       [ 64, 10,    0.00001 ],\n",
        "              7:       [ 64, 20,    0.0001  ],\n",
        "              8:       [ 64, 20,    0.00001 ],\n",
        "}\n",
        "\n",
        "\n",
        "# # batch_sizes = [16,32,64,128]\n",
        "# batch_sizes = [32,64]\n",
        "\n",
        "# # epoch_list = [10, 15, 20, 30]\n",
        "# epoch_list = [10, 20]\n",
        "\n",
        "# # learning_rates = [0.001, 0.0001 , 0.00001, 0.000001]\n",
        "# learning_rates = [0.0001 , 0.00001]\n",
        "\n",
        "# # in_sizes =  [\n",
        "# #                 [100, 100, 3],\n",
        "# #                 [136, 136, 3],\n",
        "# #                 [204, 204, 3],\n",
        "# #                 [227, 227, 3],\n",
        "# #             ]\n",
        "\n",
        "# in_sizes =  [\n",
        "#                 [136, 136, 3], #insize_1\n",
        "#                 [227, 227, 3], #insize_2\n",
        "#             ]\n",
        "\n",
        "in_sizes_dict =  {\n",
        "                      1: [136, 136, 3], #insize_1\n",
        "                      2: [227, 227, 3], #insize_2\n",
        "          \n",
        "}\n",
        "               \n",
        "            \n",
        "\n",
        "label_mode = 'binary'\n",
        "seed = 10 #changed for each fold made manually\n",
        "class_names = ['clean', 'dirty']\n",
        "\n",
        "# cnt = 0\n",
        "# for batch_size in batch_sizes:\n",
        "#   for epoch in epoch_list:\n",
        "#     for learning_rate in learning_rates:\n",
        "#       for in_size in in_sizes:\n",
        "\n",
        "#         tr_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "#                                           seed=seed, labels='inferred', image_size=in_size[:-1], \n",
        "#                                           subset = 'training', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "#         val_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "#                                                   seed=seed, labels='inferred', image_size=in_size[:-1],\n",
        "#                                                   subset = 'validation', batch_size=batch_size, validation_split=.2)\n",
        "        \n",
        "#         model_path_modified = model_path + \"_model_iteration_\" + str(cnt) + \".h5\"\n",
        "#         model_info = \"batch_size: \" + str(batch_size) + \" and epochs: \" + str(epoch) + \" and in_size: \" + str(in_size) + \" and lr: \" + str(learning_rate)\n",
        "#         print(\"The model info: \", model_info)\n",
        "\n",
        "#         SolNet_model = SolNet_updated_for_loop(in_size, learning_rate, model_path_modified)\n",
        "        \n",
        "#         history = SolNet_model.fit(tr_dataset, validation_data=val_dataset, epochs=epoch, batch_size=batch_size)\n",
        "#         model_accuracy_plot_modified, model_loss_plot_modified = plot_history(history, model_plot_path, cnt)\n",
        "\n",
        "#         test_acc, model_size, executionTime, evluation_df = evaluate_model(model_path_modified, model_info, val_dataset)\n",
        "#         print(\"The evaluation df: \\n\", evluation_df, \"\\n\")\n",
        "\n",
        "#         save_execution_trial(evluation_df, model_excel_file_path, model_accuracy_plot_modified, model_loss_plot_modified, cnt)\n",
        "\n",
        "#         cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# cnt = 0\n",
        "\n",
        "\n",
        "\n",
        "for key_1, value_1 in in_sizes_dict.items():\n",
        "\n",
        "  cnt_1 = key_1\n",
        "  in_size = value_1\n",
        "\n",
        "  for key, value in loop_dict.items():\n",
        "\n",
        "    cnt = key\n",
        "    batch_size, epoch, learning_rate = value\n",
        "\n",
        "    tr_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "                                      seed=seed, labels='inferred', image_size=in_size[:-1], \n",
        "                                      subset = 'training', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "    val_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "                                              seed=seed, labels='inferred', image_size=in_size[:-1],\n",
        "                                              subset = 'validation', batch_size=batch_size, validation_split=.2)\n",
        "    \n",
        "    model_path_modified = model_path + \"_model_iteration_\" + str(cnt) + \"__size_iteration_\" + str(cnt_1) + \".h5\"\n",
        "    model_info = \"batch_size: \" + str(batch_size) + \" and epochs: \" + str(epoch) + \" and in_size: \" + str(in_size) + \" and lr: \" + str(learning_rate)\n",
        "    print(\"The model info: \", model_info)\n",
        "\n",
        "    SolNet_model = SolNet_updated_for_loop(in_size, learning_rate, model_path_modified)\n",
        "    \n",
        "    history = SolNet_model.fit(tr_dataset, validation_data=val_dataset, epochs=epoch, batch_size=batch_size)\n",
        "    model_accuracy_plot_modified, model_loss_plot_modified = plot_history(history, model_plot_path, cnt)\n",
        "\n",
        "    test_acc, model_size, executionTime, evluation_df = evaluate_model(model_path_modified, model_info, val_dataset)\n",
        "    print(\"The evaluation df: \\n\", evluation_df, \"\\n\")\n",
        "\n",
        "    save_execution_trial(evluation_df, model_excel_file_path, model_accuracy_plot_modified, model_loss_plot_modified, str(cnt) + \"__size_iteration_\" + str(cnt_1) )\n",
        "\n",
        "    # cnt += 1"
      ],
      "metadata": {
        "id": "Rbkgdo7SdF5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0aeee3a-4f2d-4499-dfbd-5fd405f8affa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1440 files belonging to 2 classes.\n",
            "Using 1152 files for training.\n",
            "Found 1440 files belonging to 2 classes.\n",
            "Using 288 files for validation.\n",
            "The model info:  batch_size: 32 and epochs: 10 and in_size: [136, 136, 3] and lr: 0.0001\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 136, 136, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 96)        34944     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 15, 15, 256)       614656    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 15, 15, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 384)         885120    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7, 7, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 7, 384)         1327488   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 7, 7, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 256)         884992    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 7, 7, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 3, 3, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              9441280   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 4097      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,979,393\n",
            "Trainable params: 29,976,641\n",
            "Non-trainable params: 2,752\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        }
      ]
    }
  ]
}