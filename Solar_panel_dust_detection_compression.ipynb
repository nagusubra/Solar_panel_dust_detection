{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagusubra/Solar_panel_dust_detection/blob/main/Solar_panel_dust_detection_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQAMdUOp9lRn"
      },
      "source": [
        "#Install libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLxJiY6u9Zh0",
        "outputId": "263c6ab0-c46f-43c5-af0c-ebe345800e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.9/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.9/dist-packages (from openpyxl) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.9/dist-packages (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-model-optimization\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "\n",
        "import xlsxwriter\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "\n",
        "import tempfile\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qeqtoyTCppFF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WVnbwh-9x2l",
        "outputId": "0c8c3b55-c9a7-43fb-f0c5-150e372e8f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mounting google drive (if you are using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjpFrB77S6QU"
      },
      "source": [
        "# Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oB55zL05S_Oe"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model Size\n",
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "  return os.path.getsize(zipped_file)\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, model_info, val_dataset):\n",
        "\n",
        "  # Evaluate test accuracy and test loss\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "  test_loss, test_acc = model.evaluate(val_dataset, verbose=0)\n",
        "\n",
        "  # Evaluate Model Size\n",
        "  model_size = get_gzipped_model_size(model_path)\n",
        "\n",
        "  # Evaluate Inference Time\n",
        "  startTime = time.time()\n",
        "  prediction = model.predict(val_dataset)\n",
        "  executionTime = (time.time() - startTime)/len(val_dataset)\n",
        "\n",
        "  # Print\n",
        "  print('\\nModel Accuracy:', test_acc*100, '%')\n",
        "  print(\"Model Size: %.2f bytes\" % (model_size))\n",
        "  print(\"Inference Time is: \", executionTime, \"s\")\n",
        "\n",
        "  # Build Evalution dataframe\n",
        "  evulation_dict = {\n",
        "                      \"Evaluation type\": \"Evualation\",\n",
        "                      \"Model Information\": model_info,\n",
        "                      \"Accuracy\": str(test_acc*100) + \" %\",\n",
        "                      \"Loss\": str(test_loss*100) + \" %\",\n",
        "                      \"Model Size\": str(model_size) + \" bytes\",\n",
        "                      \"Inference Time\": str(executionTime) + \" sec\"\n",
        "                    }\n",
        "  \n",
        "  evulation_df = pd.DataFrame.from_dict(evulation_dict, orient='index').reset_index()\n",
        "\n",
        "\n",
        "  return test_acc, model_size, executionTime, evulation_df\n",
        "\n",
        "\n",
        "def evaluate_model_without_saving_stats(model_path, model_info, val_dataset):\n",
        "\n",
        "  # Evaluate test accuracy and test loss\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "  test_loss, test_acc = model.evaluate(val_dataset, verbose=0)\n",
        "\n",
        "  # Evaluate Model Size\n",
        "  model_size = get_gzipped_model_size(model_path)\n",
        "\n",
        "  # Evaluate Inference Time\n",
        "  startTime = time.time()\n",
        "  prediction = model.predict(val_dataset)\n",
        "  executionTime = (time.time() - startTime)/len(val_dataset)\n",
        "\n",
        "  # Print\n",
        "  print('\\nModel Accuracy:', test_acc*100, '%')\n",
        "  print(\"Model Size: %.2f bytes\" % (model_size))\n",
        "  print(\"Inference Time is: \", executionTime, \"s\")\n",
        "\n",
        "  # Build Evalution dataframe\n",
        "  evulation_dict = {\n",
        "                      \"Evaluation type\": \"Evualation\",\n",
        "                      \"Model Information\": model_info,\n",
        "                      \"Accuracy\": str(test_acc*100) + \" %\",\n",
        "                      \"Loss\": str(test_loss*100) + \" %\",\n",
        "                      \"Model Size\": str(model_size) + \" bytes\",\n",
        "                      \"Inference Time\": str(executionTime) + \" sec\"\n",
        "                    }\n",
        "  \n",
        "  evulation_df = pd.DataFrame.from_dict(evulation_dict, orient='index').reset_index()\n",
        "\n",
        "\n",
        "  return test_acc, model_size, executionTime, evulation_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# research paper evaluation function\n",
        "def evaluate_research_paper_with_model_path(model_path):\n",
        "  # model_path = \"/content/models/solnet.hdf5\"\n",
        "  solnet = load_model(model_path, compile=False)\n",
        "  history = solnet.history()\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.title('acc loss vs epoch')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'acc'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def evaluate_research_paper_with_model(model):\n",
        "  history = model.history()\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.title('acc loss vs epoch')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'acc'], loc='upper left')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfkTB4_DiYTF"
      },
      "source": [
        "# Build base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nc6lZnKUixz"
      },
      "outputs": [],
      "source": [
        "def SolNet(in_size):\n",
        "  i = Input(in_size)\n",
        "  r = Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(i)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(r)\n",
        "  r = BatchNormalization()(r)\n",
        "  r = MaxPooling2D(pool_size=(3,3), strides=(2,2))(r)\n",
        "  r = Flatten()(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  r = Dense(4096, activation='relu')(r)\n",
        "  r = Dropout(0.5)(r)\n",
        "  o = Dense(1, activation='sigmoid')(r)\n",
        "  SolNet = Model(i, o)\n",
        "  SolNet.save('models/solnet.hdf5')\n",
        "  SolNet.save('/content/drive/MyDrive/Solar_panel_dust_detection/models/base_solnet_model'+str(datetime.datetime.today().date())+'.hdf5')\n",
        "  SolNet.save('/content/drive/MyDrive/Solar_panel_dust_detection/models/base_solnet_model'+str(datetime.datetime.today().date())+'.h5')\n",
        "  SolNet.compile(optimizer=Adam(.0001, .8, .9), loss=binary_crossentropy, metrics=['acc'])\n",
        "  SolNet.summary()\n",
        "  return SolNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNHot2r1TJdo"
      },
      "outputs": [],
      "source": [
        "# base model\n",
        "\n",
        "batch_size = 32\n",
        "#location = 'dataset/'\n",
        "location = \"/content/drive/MyDrive/Solar_panel_dust_detection/dataset_1\"\n",
        "label_mode = 'binary'\n",
        "seed = 10 #changed for each fold made manually\n",
        "epochs=30\n",
        "#epochs=5\n",
        "\n",
        "\n",
        "class_names = ['clean', 'dirty']\n",
        "in_size = [227, 227, 3]\n",
        "\n",
        "tr_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "                                          seed=seed, labels='inferred', image_size=in_size[:-1], \n",
        "                                          subset = 'training', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "val_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "                                          seed=seed, labels='inferred', image_size=in_size[:-1],\n",
        "                                          subset = 'validation', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "in_size = [227, 227, 3]\n",
        "SolNet = SolNet(in_size)\n",
        "\n",
        "\n",
        "\n",
        "startTime = time.time()\n",
        "history = SolNet.fit(tr_dataset, validation_data=val_dataset, epochs=epochs, batch_size=batch_size)\n",
        "executionTime = (time.time() - startTime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4C6_Bb-Yi7w"
      },
      "outputs": [],
      "source": [
        "print(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJOz2CyMbpef"
      },
      "outputs": [],
      "source": [
        "# research paer's evaluation function\n",
        "evaluate(\"/content/drive/MyDrive/Solar_panel_dust_detection/models/base_solnet_model.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgI6kD7CZIm4"
      },
      "outputs": [],
      "source": [
        "test_acc, model_size, executionTime, evulation_df = evaluate_model(\"/content/drive/MyDrive/Solar_panel_dust_detection/models/base_solnet_model.hdf5\", \"#0\", val_dataset)\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei6SYOpNYk0n"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['acc'], label='accuracy')\n",
        "plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
        "plt.title('model accuracy plot')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss plot')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lFSom8Hg-ut"
      },
      "outputs": [],
      "source": [
        "print(\"Inference Time is\", executionTime/60, \"mins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load base model and Evaluate\n"
      ],
      "metadata": {
        "id": "LSh8r6pSvbTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = '/content/drive/MyDrive/Solar_panel_dust_detection/final_base_model/solnetOriginal_final_base_model.hdf5'"
      ],
      "metadata": {
        "id": "PxxCx3C9vetW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base model\n",
        "\n",
        "batch_size = 32\n",
        "#location = 'dataset/'\n",
        "location = \"/content/drive/MyDrive/Solar_panel_dust_detection/dataset_1\"\n",
        "label_mode = 'binary'\n",
        "seed = 10 #changed for each fold made manually\n",
        "epochs=30\n",
        "\n",
        "class_names = ['clean', 'dirty']\n",
        "in_size = [227, 227, 3]\n",
        "\n",
        "tr_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "                                          seed=seed, labels='inferred', image_size=in_size[:-1], \n",
        "                                          subset = 'training', batch_size=batch_size, validation_split=.2)\n",
        "\n",
        "val_dataset = image_dataset_from_directory(directory=location, label_mode= label_mode, class_names=class_names,\n",
        "                                          seed=seed, labels='inferred', image_size=in_size[:-1],\n",
        "                                          subset = 'validation', batch_size=batch_size, validation_split=.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADxrshiBxYPD",
        "outputId": "f7b93e13-0b64-4a23-d8a2-12ff0c33c5f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1440 files belonging to 2 classes.\n",
            "Using 1152 files for training.\n",
            "Found 1440 files belonging to 2 classes.\n",
            "Using 288 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, model_size, executionTime, evulation_df = evaluate_model_without_saving_stats(final_model_path, \"#0\", val_dataset)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4ptHfowaOW",
        "outputId": "92cfa092-30bd-4e78-a924-76d71b914b07"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 43s 2s/step\n",
            "\n",
            "Model Accuracy: 72.56944179534912 %\n",
            "Model Size: 659509171.00 bytes\n",
            "Inference Time is:  9.118888801998562 s\n",
            "0.7256944179534912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evulation_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4fTy3GU83zDO",
        "outputId": "ef798f87-a329-49cd-8a9f-d3c2f8a03d99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               index                      0\n",
              "0    Evaluation type             Evualation\n",
              "1  Model Information                     #0\n",
              "2           Accuracy    72.56944179534912 %\n",
              "3               Loss     261.795973777771 %\n",
              "4         Model Size        659509171 bytes\n",
              "5     Inference Time  9.118888801998562 sec"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c773c0c-c101-433d-9c53-eaf1b834d875\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Evaluation type</td>\n",
              "      <td>Evualation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Model Information</td>\n",
              "      <td>#0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>72.56944179534912 %</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Loss</td>\n",
              "      <td>261.795973777771 %</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Model Size</td>\n",
              "      <td>659509171 bytes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Inference Time</td>\n",
              "      <td>9.118888801998562 sec</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c773c0c-c101-433d-9c53-eaf1b834d875')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c773c0c-c101-433d-9c53-eaf1b834d875 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c773c0c-c101-433d-9c53-eaf1b834d875');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}